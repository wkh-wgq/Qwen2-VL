fastapi
uvicorn
qwen-vl-utils==0.0.2
transformers-stream-generator==0.0.4
torch==2.4.0
torchvision==0.19.0
git+https://github.com/huggingface/transformers.git
accelerate
ninja
# 需要手动安装一下这个，因为不知道要怎么添加带参数的，用于attn_implementation="flash_attention_2"
# pip install flash-attn --no-build-isolation